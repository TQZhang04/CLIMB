defaults: 
  - base_config
  - _self_
  - dataset: strict_small 
  - objective_curriculum: base_mlm
  - model: roberta_pln_small
  - tokenizer: cbt_8192

dataset: 
  name: 'cambridge-climb/BabyLM'

model:
  model_kwargs:
    vocab_size: 8192

experiment: 
  seed: 42 
  group: "climb-replication"
  name: "climb-data"

data_preprocessing:
  include_punctuation: True
  join_sentences: True
  max_input_length: 128

trainer: 
  batch_size: 16 # across 4 GPUs gives an effective batch size of 128
  lr: 1e-3 # 1e-4 is used in fairseq; 1e-3 is default in huggingface
  num_warmup_steps: 1_000
  max_training_steps: 20_000
  eval_blimp: False
  eval_glue: False
  eval_msgs: False
  eval_perplexity: True

data_curriculum:
  difficulty_scorer_name: "data_split"
  difficulty_scorer_kwargs: {"spoken_first": False,
                             "uniform_sampling": True}
  pacing_fn_name: "linear"
  pacing_fn_kwargs: {"start_percent": 0.0625, # Start after 25k steps
                    "end_percent": 0.875,
                    "starting_difficulty": 0.1,
                    "max_difficulty": 1.0}